{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "This codelab is based on https://www.rainbowbreeze.it/serving-gemma-2b-for-free-using-google-colab/ blogpost.\n",
        "\n",
        "Author: Alfredo Morresi"
      ],
      "metadata": {
        "id": "XiD105FHhkFD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw3X0vRfxkpe"
      },
      "source": [
        "## Install Ollama inside this Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnJKCbD0xnP_"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QPAyIXpyGdw"
      },
      "source": [
        "### Download packages to create the Cloudflare tunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8EdCNVYyPq7"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6N215xhzgRE"
      },
      "source": [
        "### Prepare the thread to expose the Ollama via cloudflare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhWEzisT7s7n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Set OLLAMA_HOST to specify bind address\n",
        "# https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux\n",
        "os.environ.update({'OLLAMA_HOST': '0.0.0.0'})\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "\n",
        "def iframe_thread(port):\n",
        "    print(\"Entro nel thread\")\n",
        "    while True:\n",
        "        time.sleep(0.5)\n",
        "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        result = sock.connect_ex(('127.0.0.1', port))\n",
        "        if result == 0:\n",
        "            break\n",
        "        sock.close()\n",
        "\n",
        "    print(\"Esco dal thread\")\n",
        "    p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", f\"http://127.0.0.1:{port}\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    for line in p.stderr:\n",
        "        l = line.decode()\n",
        "        if \"trycloudflare.com \" in l:\n",
        "            print(\"\\n\\n\\n\\n\\n\")\n",
        "            print(\"running ollama server\\n\\n\", l[l.find(\"http\"):], end='')\n",
        "            print(\"\\n\\n\\n\\n\\n\")\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(11434,)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuulwOQkzKF1"
      },
      "source": [
        "### Start serving Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7O6jACnzTHi"
      },
      "outputs": [],
      "source": [
        "# Once the serve command is launched, the Colab waits indefinitely here for new command\n",
        "!ollama serve"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Ollama API from the command line\n",
        "\n",
        "Finally, it's time to call the exposed Ollama API, with ```<your_cloudflare_url>``` as something like https://auditor-protect-vids-slots.trycloudflare.com.\n",
        "\n",
        "\n",
        "Install the gemma:2b model\n",
        "```\n",
        "curl <your_cloudflare_url>/api/pull -d '{ \"name\": \"gemma:2b\" }'\n",
        "```\n",
        "\n",
        "Cache the model, to save some time between calls (by default, after 5 mins the model is discarded from memory)\n",
        "```\n",
        "curl <your_cloudflare_url>/api/generate -d '{\"model\": \"gemma:2b\", \"keep_alive\": -1}'\n",
        "```\n",
        "\n",
        "Finally, ask for the prompt\n",
        "```\n",
        "curl <your_cloudflare_url>/api/generate --max-time 600 -d '{\"model\": \"gemma:2b\", \"stream\":false, \"prompt\": \"generate a bash script to ask the user for a url, for a message, and then call that url using the message as json payload\"}' | jq \".response\"\n",
        "```"
      ],
      "metadata": {
        "id": "qXCn23mVgl2X"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}